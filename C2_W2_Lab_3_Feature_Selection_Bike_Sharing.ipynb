{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C2_W2_Lab_3_Feature_Selection_Bike_Sharing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabioantonini/Applied-Data-Science-Coursera-Capstone/blob/master/C2_W2_Lab_3_Feature_Selection_Bike_Sharing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgyeUA40aslE"
      },
      "source": [
        "# Ungraded Lab: Feature Selection Bike Sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Optzx97aahnJ"
      },
      "source": [
        "Feature selection involves picking the set of features that are most relevant to the target variable. This helps in reducing the complexity of your model, as well as minimizing the resources required for training and inference. This has greater effect in production models where you maybe dealing with terabytes of data or serving millions of requests.\n",
        "\n",
        "In this notebook, you will run through the different techniques in performing feature selection on the [Seoul Bike Sharing](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand). Most of the modules will come from [scikit-learn](https://scikit-learn.org/stable/), one of the most commonly used machine learning libraries. It features various machine learning algorithms and has built-in implementations of different feature selection methods. Using these, you will be able to compare which method works best for this particular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEnMK4DRNV1O"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZersTw6TH1Zj"
      },
      "source": [
        "# for data processing and manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# scikit-learn modules for feature selection and model evaluation\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# libraries for visualization\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyM1tqLgAMdo"
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "else:\n",
        "  print('Not running on CoLab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDCNGDZ4vaEA"
      },
      "source": [
        "### Preview the  dataset\n",
        "\n",
        "You will be using the [Seoul Bike Sharing](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand).\n",
        "\n",
        "Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n",
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information. The label (target) is the 'Rented Bike count'.\n",
        "\n",
        "Here is the description of the features again: \n",
        "\n",
        "* **Date** : year-month-day\n",
        "* **Rented Bike count** - Count of bikes rented at each hour\n",
        "* **Hour** - Hour of the day\n",
        "* **Temperature** - Temperature in Celsius\n",
        "* **Humidity** - %\n",
        "* **Windspeed** - m/s\n",
        "* **Visibility** - 10m\n",
        "* **Dew point temperature** - Celsius\n",
        "* **Solar radiation** - MJ/m2\n",
        "* **Rainfall** - mm\n",
        "* **Snowfall** - cm\n",
        "* **Seasons** - Winter, Spring, Summer, Autumn\n",
        "* **Holiday** - Holiday/No holiday\n",
        "* **Functional Day** - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvMpn0VaazcC"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "We've already downloaded the CSV in your workspace. Run the cell below to load it in the lab environment and inspect its properties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DspE2DYYPpRp"
      },
      "source": [
        "# Load the dataset\n",
        "link=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv\"\n",
        "df = pd.read_csv(link, encoding = 'latin1')\n",
        "\n",
        "# Print datatypes\n",
        "print(df.dtypes)\n",
        "\n",
        "# Describe columns\n",
        "df.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXQUEXpivaED"
      },
      "source": [
        "# Preview the dataset\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkXWz0WnbEPd"
      },
      "source": [
        "## Remove Unwanted Features\n",
        "\n",
        "You can remove features that are not needed when making predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73EVqL6_N2-T"
      },
      "source": [
        "# Check if there are null values in any of the columns.\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hmvhwV2vaEG"
      },
      "source": [
        "There is no Null value and all the features are numerical unless 'Date'. We can move ahead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb5NF4L8vaEH"
      },
      "source": [
        "## Removing 'Date' feature\n",
        "The feature 'Date' will be removed because not helpful to train the model by Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7KsS8cDvaEH"
      },
      "source": [
        "# Drop the previous string column\n",
        "#df.drop(['Date'], axis=1, inplace=True)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Date'] = df['Date'].map(dt.datetime.toordinal)\n",
        "# Preview the dataset after che change to the date\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ9yk-r6bYdZ"
      },
      "source": [
        "## Integer Encode Holiday feature\n",
        "\n",
        "You may have realized that the target column, `Holiday`, is encoded as a string type categorical variable: 'No Holiday' and 'Holiday'. You need to convert these into integers before training the model. Since there are only two classes, you can use `0` for 'No Holiday' and `1` for 'Holiday'. Let's create a column `Holiday_int` containing this integer representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPDxg0AO4g-N"
      },
      "source": [
        "# Integer encode the target variable, Holiday\n",
        "df[\"Holiday_int\"] = (df[\"Holiday\"] == 'Holiday').astype('int')\n",
        "\n",
        "# Drop the previous string column\n",
        "df.drop(['Holiday'], axis=1, inplace=True)\n",
        "\n",
        "# Check the new column\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5tJWyLavaEJ"
      },
      "source": [
        "## Integer Encode 'Functioning Day' feature\n",
        "\n",
        "The feature `Functioning Day`, is encoded as a string type categorical variable: 'Yes' and 'No'. You need to convert these into integers before training the model. Since there are only two classes, you can use `0` for 'No' and `1` for 'Yes'. Let's create a column `Functioning_Day_int` containing this integer representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZR2_d6bvaEK"
      },
      "source": [
        "# Integer encode the target variable, Holiday\n",
        "df[\"Functioning_Day_int\"] = (df[\"Functioning Day\"] == 'Yes').astype('int')\n",
        "\n",
        "# Drop the previous string column\n",
        "df.drop(['Functioning Day'], axis=1, inplace=True)\n",
        "\n",
        "# Check the new column\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvECom0FvaEL"
      },
      "source": [
        "## Integer Encode Seasons feature\n",
        "\n",
        "You may have realized that the target column `Seasons` is encoded as a string type categorical variable with domain 'Autumn', 'Spring', 'Summer', 'Winter'. You need to convert these into integers before training the model. The mapping will be:\n",
        "* 'Spring' -> 0\n",
        "* 'Summer' -> 1\n",
        "* 'Autumn' -> 2\n",
        "* 'Winter' -> 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfT7ga3EvaEM"
      },
      "source": [
        "df['Seasons'] = df['Seasons'].replace(['Spring','Summer','Autumn', 'Winter'], [0, 1, 2,3])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln009ORbvaEN"
      },
      "source": [
        "# Print datatypes\n",
        "print(df.dtypes)\n",
        "\n",
        "# Describe columns\n",
        "df.describe(include='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s08Owp2kb0SB"
      },
      "source": [
        "## Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vQK6ipnbmf8"
      },
      "source": [
        "Next, split the dataset into feature vectors `X` and target vector (Rented Bike Count) `Y` to fit a [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn.linear_model.LinearRegression). You will then compare the performance of each feature selection technique, using [mse](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html?highlight=mean_squared_error#sklearn.metrics.mean_squared_error), [mae](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html?highlight=mean_absolute_error#sklearn.metrics.mean_absolute_error) as evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTuyLttI5h0w"
      },
      "source": [
        "# Split feature and target vectors\n",
        "X = df.drop(\"Rented Bike Count\", 1)\n",
        "Y = df[\"Rented Bike Count\"]\n",
        "Y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uULmswIiThX"
      },
      "source": [
        "### Fit the Model and Calculate Metrics\n",
        "\n",
        "You will define helper functions to train your model and use the scikit-learn modules to evaluate your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JVl3UGpq7_I"
      },
      "source": [
        "def fit_model(X, Y):\n",
        "    '''Use a linear_model.LinearRegression() for this problem.'''\n",
        "    \n",
        "    # define the model to use\n",
        "    model = linear_model.LinearRegression()\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X, Y)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg-QoSiErLgv"
      },
      "source": [
        "def calculate_metrics(model, X_test_scaled, Y_test):\n",
        "    '''Get model evaluation metrics on the test set.'''\n",
        "    \n",
        "    # Get model predictions\n",
        "    y_predict_r = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate evaluation metrics for assessing performance of the model.\n",
        "    mse = mean_squared_error(Y_test, y_predict_r)\n",
        "    mae = mean_absolute_error(Y_test, y_predict_r)\n",
        "\n",
        "    return mse, mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F06PrANXrLrL"
      },
      "source": [
        "def train_and_get_metrics(X, Y):\n",
        "    '''Train a Linear Regression and get evaluation metrics'''\n",
        "    \n",
        "    # Split train and test sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 123)\n",
        "\n",
        "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Call the fit model function to train the model on the normalized features and the 'Rented Bike Count' values\n",
        "    model = fit_model(X_train_scaled, Y_train)\n",
        "\n",
        "    # Make predictions on test dataset and calculate metrics.\n",
        "    mse, mae = calculate_metrics(model, X_test_scaled, Y_test)\n",
        "    \n",
        "    return mse, mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdOOXiqSmH6p"
      },
      "source": [
        "def evaluate_model_on_features(X, Y):\n",
        "    '''Train model and display evaluation metrics.'''\n",
        "    \n",
        "    # Train the model, predict values and get metrics\n",
        "    mse, mae = train_and_get_metrics(X, Y)\n",
        "\n",
        "    # Construct a dataframe to display metrics.\n",
        "    display_df = pd.DataFrame([[mse, mae]], columns=['MSE', 'MAE'])\n",
        "    \n",
        "    return display_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8A0pEIZiZka"
      },
      "source": [
        "Now you can train the model with all features included then calculate the metrics. This will be your baseline and you will compare this to the next outputs when you do feature selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sXRVKV-nlwR"
      },
      "source": [
        "# Calculate evaluation metrics\n",
        "all_features_eval_df = evaluate_model_on_features(X, Y)\n",
        "all_features_eval_df.index = ['All features']\n",
        "\n",
        "# Initialize results dataframe\n",
        "results = all_features_eval_df\n",
        "\n",
        "# Check the metrics\n",
        "results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g--wIHmFBjgr"
      },
      "source": [
        "## Correlation Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ui-8O8Li7dY"
      },
      "source": [
        "It is a good idea to calculate and visualize the correlation matrix of a data frame to see which features have high correlation. You can do that with just a few lines as shown below. The Pandas [corr()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) method computes the Pearson correlation by default and you will plot it with Matlab PyPlot and Seaborn. The darker blue boxes show features with high positive correlation while white ones indicate high negative correlation. The diagonals will have 1's because the feature is mapped on to itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8rBZqEfw45p"
      },
      "source": [
        "# Set figure size\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "# Calculate correlation matrix\n",
        "cor = df.corr() \n",
        "\n",
        "# Plot the correlation matrix\n",
        "sns.heatmap(cor, annot=True, cmap=plt.cm.PuBu)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZHg3wdKjDWX"
      },
      "source": [
        "## Filter Methods\n",
        "\n",
        "Let's start feature selection with filter methods. This type of feature selection uses statistical methods to rank a given set of features. Moreover, it does this ranking regardless of the model you will be training on (i.e. you only need the feature values). When using these, it is important to note the types of features and target variable you have. Here are a few examples:\n",
        "\n",
        "* Pearson Correlation (numeric features - numeric target, *exception: when target is 0/1 coded*)\n",
        "* ANOVA f-test (numeric features - categorical target)\n",
        "* Chi-squared (categorical features - categorical target)\n",
        "\n",
        "Let's use some of these in the next cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Ybif0ivaET"
      },
      "source": [
        "### Correlation with the target variable\n",
        "\n",
        "Let's start by determining which features are strongly correlated with the 'Rented Bike Count' (i.e. the target variable). Since we have numeric features and our target, although categorical, is 0/1 coded, we can use Pearson correlation to compute the scores for each feature. This is also categorized as *supervised* feature selection because we're taking into account the relationship of each feature with the target variable. Moreover, since only one variable's relationship to the target is taken at a time, this falls under *univariate feature selection*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m34lMYEy9MK"
      },
      "source": [
        "# Get the absolute value of the correlation\n",
        "cor_target = abs(cor[\"Rented Bike Count\"])\n",
        "\n",
        "# Select highly correlated features (thresold = 0.2)\n",
        "relevant_features = cor_target[cor_target>0.2]\n",
        "\n",
        "# Collect the names of the features\n",
        "names = [index for index, value in relevant_features.iteritems()]\n",
        "\n",
        "# Drop the target variable from the results\n",
        "names.remove('Rented Bike Count')\n",
        "\n",
        "# Display the results\n",
        "print(names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBoG4iMfjKzp"
      },
      "source": [
        "Now try training the model again but only with the features in the columns you just gathered. You can observe that there is an improvement in the metrics compared to the model you trained earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiHBfYEc8Wqb"
      },
      "source": [
        "# Evaluate the model with new features\n",
        "strong_features_eval_df = evaluate_model_on_features(df[names], Y)\n",
        "strong_features_eval_df.index = ['Strong features']\n",
        "\n",
        "# Append to results and display\n",
        "results = results.append(strong_features_eval_df)\n",
        "results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6q3QJIrje2S"
      },
      "source": [
        "### Correlation with other features\n",
        "\n",
        "You will now eliminate features which are highly correlated with each other. This helps remove redundant features thus resulting in a simpler model. Since the scores are calculated regardless of the target variable, this can be categorized under *unsupervised* feature selection.\n",
        "\n",
        "For this, you will plot the correlation matrix of the features selected previously. Let's first visualize the correlation matrix again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yIfyep00eQb"
      },
      "source": [
        "# Set figure size\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "# Calculate the correlation matrix for target relevant features that you previously determined\n",
        "new_corr = df[names].corr()\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "sns.heatmap(new_corr, annot=True, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkNIwzvEvaEV"
      },
      "source": [
        "You will see that `Temperature(°C)` is highly correlated to `Dew point temperature(°C)`. You can retain `Temperature` and remove the rest of the features highly correlated to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTXGrmiqkS-b"
      },
      "source": [
        "This is a more magnified view of the features that are highly correlated to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrzX3gpwwfKB"
      },
      "source": [
        "# Set figure size\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "# Select a subset of features\n",
        "new_corr = df[['Temperature(°C)', 'Dew point temperature(°C)']].corr()\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "sns.heatmap(new_corr, annot=True, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSCK7aYOkZNf"
      },
      "source": [
        "You will now evaluate the model on the features selected based on your observations. You can see that the metrics show the same values as when it was using all the features. This indicates that you can get the same model performance even if you reduce the number of features. In other words, the 4 features you removed were indeed redundant and you only needed the ones you retained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlDWwAZi8LPi"
      },
      "source": [
        "# Remove the features with high correlation to other features\n",
        "subset_feature_corr_names = [x for x in names if x not in ['Temperature(°C)', 'Dew point temperature(°C)']]\n",
        "\n",
        "# Calculate and check evaluation metrics\n",
        "subset_feature_eval_df = evaluate_model_on_features(df[subset_feature_corr_names], Y)\n",
        "subset_feature_eval_df.index = ['Subset features']\n",
        "\n",
        "# Append to results and display\n",
        "results = results.append(subset_feature_eval_df)\n",
        "results.head(n=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JidOooxzvaEW"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "That's it for this quick rundown of the different feature selection methods. As shown, you can do quick experiments with these because convenience modules are already available in libraries like sci-kit learn. It is a good idea to do this preprocessing step because not only will you save resources, you may even get better results than when you use all features. Try it out on your previous/upcoming projects and see what results you get!"
      ]
    }
  ]
}